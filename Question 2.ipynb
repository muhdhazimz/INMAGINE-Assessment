{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Sq1g1IO6DUd",
    "outputId": "16b3c043-21b6-4bc5-cee9-0eec149017e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Frame 0: a bunch of flowers in a vase\n",
      "Frame 3: a bunch of flowers in a vase\n",
      "Frame 6: a bunch of flowers in a vase\n",
      "Frame 8: a bunch of flowers in a vase\n",
      "Frame 10: a bunch of flowers in a vase\n",
      "Frame 12: a bunch of flowers in a vase\n",
      "Frame 14: a bunch of flowers in a vase\n",
      "Frame 16: a bunch of flowers in a vase\n",
      "Frame 18: a bunch of flowers in a vase\n",
      "Frame 20: a bunch of flowers in a vase\n",
      "Frame 22: a bunch of flowers in a vase\n",
      "Frame 24: a bunch of flowers in a vase\n",
      "Frame 26: a bunch of flowers in a vase\n",
      "Frame 28: a bunch of flowers in a vase\n",
      "Frame 30: a bunch of flowers in a vase\n",
      "Frame 32: a bunch of flowers in a vase\n",
      "Frame 34: a bunch of flowers in a vase\n",
      "Frame 36: a bunch of flowers in a vase\n",
      "Frame 38: a bunch of flowers in a vase\n",
      "Frame 40: a bouquet of flowers on a table\n",
      "Frame 42: a bunch of flowers in a vase\n",
      "Frame 44: a bouquet of flowers in a vase\n",
      "Frame 46: a bouquet of flowers on a table\n",
      "Frame 48: a bouquet of flowers on a table\n",
      "Frame 50: a vase filled with flowers and a candle\n",
      "Frame 52: a bunch of flowers in a vase\n",
      "Frame 54: a vase filled with flowers and greene\n",
      "Frame 56: a bunch of flowers in a vase\n",
      "Frame 58: a bunch of flowers in a vase\n",
      "Frame 60: a vase filled with flowers on top of a table\n",
      "Frame 62: a vase filled with flowers on top of a table\n",
      "Frame 64: a vase filled with flowers and a candle\n",
      "Frame 66: a vase filled with flowers and a candle\n",
      "Frame 68: a vase filled with flowers and a candle\n",
      "Frame 70: a vase filled with flowers and a candle\n",
      "Frame 72: a vase filled with flowers and a candle\n",
      "Frame 74: a vase filled with flowers and a candle\n",
      "Frame 76: a vase filled with flowers and a candle\n",
      "Frame 77: a vase filled with flowers and a candle\n",
      "Frame 79: a vase filled with flowers and a white flower\n",
      "Frame 80: a vase filled with flowers and a candle\n",
      "Frame 82: a vase filled with flowers and a candle\n",
      "Frame 84: a vase filled with flowers and greene\n",
      "Frame 85: a vase filled with flowers and greene\n",
      "Frame 86: a vase filled with flowers and greene\n",
      "Frame 87: a vase filled with flowers on top of a table\n",
      "Frame 88: a vase filled with flowers and greene\n",
      "Frame 89: a vase filled with white flowers and pink flowers\n",
      "Frame 90: a vase filled with flowers and a candle\n",
      "Frame 91: a vase filled with white flowers and pink flowers\n",
      "Frame 92: a vase filled with white flowers and pink flowers\n",
      "Frame 93: a vase filled with white flowers and pink flowers\n",
      "Frame 94: a vase filled with white flowers and pink flowers\n",
      "Frame 95: a vase filled with white flowers and pink roses\n",
      "Frame 96: a vase filled with white flowers and pink roses\n",
      "Frame 97: a vase filled with white flowers and pink roses\n",
      "Frame 98: a vase filled with white flowers and pink roses\n",
      "Frame 99: a vase filled with white flowers and pink flowers\n",
      "Frame 100: a vase filled with flowers and a white flower\n",
      "Frame 101: a vase filled with white flowers and pink flowers\n",
      "Frame 102: a vase filled with white and pink flowers\n",
      "Frame 103: a vase filled with white flowers and pink flowers\n",
      "Frame 104: a vase filled with flowers and greene\n",
      "Frame 105: a vase filled with white flowers and pink flowers\n",
      "Frame 106: a bouquet of flowers on a table\n",
      "Frame 107: a bouquet of flowers on a table\n",
      "Frame 108: a vase filled with white and pink flowers\n",
      "Frame 109: a vase filled with flowers and greene\n",
      "Frame 110: a vase filled with flowers and greene\n",
      "Frame 111: a vase filled with flowers and greene\n",
      "Frame 112: a bouquet of flowers is shown in this image\n",
      "Frame 113: a bouquet of flowers is shown in this image\n",
      "Frame 114: a bouquet of flowers is shown in this image\n",
      "Frame 115: a bouquet of flowers on a table\n",
      "Frame 116: a bouquet of flowers is shown in this image\n",
      "Frame 117: a bouquet with flowers in it\n",
      "Frame 118: a bouquet of flowers is shown in this image\n",
      "Frame 119: a bouquet with flowers in it\n",
      "Frame 120: a bouquet with flowers in it\n",
      "Frame 121: a bouquet of flowers in a vase\n",
      "Frame 122: a bouquet with flowers in it\n",
      "Frame 123: a bouquet of flowers in a vase\n",
      "Frame 124: a bouquet of flowers in a vase\n",
      "Frame 125: a bouquet of flowers in a vase\n",
      "Frame 126: a vase filled with flowers and greene\n",
      "Frame 127: a vase filled with flowers and greene\n",
      "Frame 128: a vase filled with flowers and greene\n",
      "Frame 129: a vase filled with flowers and greene\n",
      "Frame 130: a bouquet of flowers on a table\n",
      "Frame 131: a bouquet of flowers is shown in this image\n",
      "Frame 132: a bouquet of flowers is shown in this video\n",
      "Frame 133: a bouquet of flowers is shown in this video\n",
      "Frame 134: a bouquet of flowers is shown in this video\n",
      "Frame 135: a bouquet of flowers is shown in this video\n",
      "Frame 136: a vase filled with flowers and a video player\n",
      "Frame 137: a vase filled with flowers and a video player\n",
      "Frame 138: a vase filled with flowers and a video player\n",
      "Frame 139: a bouquet of flowers is shown in this video\n",
      "Frame 140: a bouquet of flowers with a video player\n",
      "Frame 141: a bouquet of flowers is shown in this video\n",
      "Frame 142: a bouquet of flowers with a video player\n",
      "Frame 143: a vase filled with flowers and a video player\n",
      "Frame 144: a bouquet of flowers is shown in this video\n",
      "Frame 145: a vase filled with flowers and a video player\n",
      "Frame 146: a vase filled with flowers and a video player\n",
      "Frame 147: a bunch of flowers sitting on top of a table\n",
      "Frame 148: a bouquet of flowers is shown in this video\n",
      "Frame 149: a bunch of flowers sitting on top of a table\n",
      "Frame 150: a bunch of flowers in a vase\n",
      "Frame 151: a bunch of flowers in a vase\n",
      "Frame 152: a bunch of flowers in a vase\n",
      "Frame 153: a bunch of flowers in a vase\n",
      "Frame 154: a bunch of flowers with a video player\n",
      "Frame 155: a bunch of flowers in a vase\n",
      "Frame 156: a bunch of flowers in a vase\n",
      "Frame 157: a bunch of flowers with a video player\n",
      "Frame 158: a bunch of flowers in a vase\n",
      "Frame 159: a bunch of flowers in a vase\n",
      "Frame 160: a bunch of flowers in a vase\n",
      "Frame 161: a bouquet of flowers is shown in this video\n",
      "Frame 162: a bunch of flowers sitting on top of a table\n",
      "Frame 163: a bunch of flowers sitting on top of a table\n",
      "Frame 164: a bunch of flowers sitting on top of a table\n",
      "Frame 165: a bunch of flowers in a vase\n",
      "Frame 166: a bunch of flowers in a vase\n",
      "Frame 167: a bunch of flowers sitting on top of a table\n",
      "Frame 168: a bunch of flowers in a vase\n",
      "Frame 169: a bunch of flowers sitting on top of a table\n",
      "Frame 170: a bunch of flowers sitting on top of a table\n",
      "Frame 171: a bunch of flowers sitting on top of a table\n",
      "Frame 172: a bunch of flowers sitting on top of a table\n",
      "Frame 173: a bunch of flowers in a vase\n",
      "Frame 174: a bunch of flowers sitting on top of a table\n",
      "Frame 175: a bunch of flowers sitting on top of a table\n",
      "Frame 176: a bunch of flowers sitting on top of a table\n",
      "Frame 177: a bunch of flowers in a vase\n",
      "Frame 178: a bouquet of flowers is shown in this video\n",
      "Frame 179: a vase filled with flowers and a video player\n",
      "Frame 180: a vase filled with flowers and a video player\n",
      "Frame 181: a vase filled with flowers and a video player\n",
      "Frame 182: a vase filled with flowers and a video player\n",
      "Frame 183: a bunch of flowers in a vase\n",
      "Frame 184: a bunch of flowers in a vase\n",
      "- bouquet\n",
      "Final Descriptive Keywords: - bouquet\n",
      "Final Captions: 1. The video shows a bouquet of flowers in a vase on a table. The flowers are in full bloom and have a vibrant color. The vase is placed on a table with a white cloth on it. The table is in a room with a window and a door. The video shows the flowers in different angles and\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "max_seq_length = 4096\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model_llm, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model_llm)\n",
    "\n",
    "video_path = '/content/footage_113622770.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "frame_count = 0\n",
    "captions = []\n",
    "frame_context = []\n",
    "\n",
    "def are_frames_similar(frame1, frame2, threshold=0.95):\n",
    "    # Convert frames to grayscale\n",
    "    frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    score, _ = ssim(frame1_gray, frame2_gray, full=True)\n",
    "\n",
    "    return score > threshold\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if prev_frame is not None and are_frames_similar(prev_frame, frame):\n",
    "        frame_count += 1\n",
    "        continue\n",
    "\n",
    "    img_filename = f\"frame_{frame_count}.jpg\"\n",
    "    cv2.imwrite(img_filename, frame)\n",
    "\n",
    "    image = Image.open(img_filename)\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    captions.append(caption)\n",
    "    print(f\"Frame {frame_count}: {caption}\")\n",
    "\n",
    "    prev_frame = frame\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcGjEmo2Ui59",
    "outputId": "13f2bc81-5391-4638-e871-aee124980b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Descriptive Keywords: flowers, vase, bouquet, table, candle, greene, white, pink, roses, player, sitting, video\n",
      "Final Captions: The video shows a bunch of flowers in a vase. The vase is filled with a bouquet of flowers. The flowers are in a variety of colors, including white, pink, and green. The flowers are arranged in a bouquet on a table. The table is in a room with a video player. The video player is\n"
     ]
    }
   ],
   "source": [
    "def filtered_captions(captions):\n",
    "  filtered_captions = []\n",
    "  for i, frame in enumerate(captions):\n",
    "      if i == 0 or frame != captions[i - 1]:\n",
    "          filtered_captions.append(frame)\n",
    "  return filtered_captions\n",
    "\n",
    "def generate_keywords_with_llm(filtered_captions):\n",
    "    context = \" \".join(captions)\n",
    "    alpaca_prompt = \"\"\"\n",
    "I have the following descriptions of video frames, each representing a unique scene:\n",
    "\n",
    "{context}\n",
    "\n",
    "From these descriptions, generate a list of distinct, descriptive keywords that best represent the content of the video frames. The keywords should reflect the primary elements, actions, and objects in the scenes described. Ensure that the keywords are specific and avoid redundancy. The list should include things like objects, actions, environments, and any other important details from the captions.\n",
    "Please output the keywords as a LIST of singular word, and ENSURE THAT the keywords are DISTINCT and NOT REPEATED. Make sure the words are NOT THE SAME AND REDUNDANT.\n",
    "\n",
    "Descriptive keywords:\n",
    "\"\"\"\n",
    "    formatted_prompt = alpaca_prompt.format(context=context)\n",
    "\n",
    "    inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model_llm.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "    pattern = r\"Descriptive keywords:\\s*(.*)\"\n",
    "\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    if match:\n",
    "        keywords = match.group(1)\n",
    "    else:\n",
    "        print(\"No match found.\")\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "final_keywords = generate_keywords_with_llm(filtered_captions(captions))\n",
    "print(\"Final Descriptive Keywords:\", final_keywords)\n",
    "\n",
    "def generate_captions_with_llm(filtered_captions):\n",
    "    context = \" \".join(captions)\n",
    "    alpaca_prompt = \"\"\"\n",
    "I have the following descriptions of video frames, each representing a unique scene:\n",
    "\n",
    "{context}\n",
    "\n",
    "From these descriptions, generate a caption for the whole duration of the video that best represent the content of the video frames. The caption should reflect the primary elements, actions, and objects in the scenes described.\n",
    "Please output the caption in a text format. Make sure it is desciptive and tells a story about the video in 2 sentences ONLY in a paragraph.\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "    formatted_prompt = alpaca_prompt.format(context=context)\n",
    "\n",
    "    inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model_llm.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    pattern = r\"Response:\\s*(.*)\"\n",
    "\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    if match:\n",
    "        keywords = match.group(1)\n",
    "    else:\n",
    "        print(\"No match found.\")\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "final_captions = generate_captions_with_llm(filtered_captions(captions))\n",
    "print(\"Final Captions:\", final_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "MKBKVGd7BIuJ",
    "outputId": "242b80d7-b64a-4502-fee3-52b9d4f218ea"
   },
   "outputs": [],
   "source": [
    "pip install gradio transformers langchain opencv-python openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "K4ZAPTi0BXYo",
    "outputId": "d5b27239-726d-4ed1-9684-69d4f54f846a"
   },
   "outputs": [],
   "source": [
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "saraswathi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
